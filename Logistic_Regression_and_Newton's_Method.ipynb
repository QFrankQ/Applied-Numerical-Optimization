{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QFrankQ/Applied-Numerical-Optimization/blob/main/Logistic_Regression_and_Newton's_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPSqaNgZ5Cht"
      },
      "source": [
        "#**Logistic Regression**#\n",
        "To help you better understand the pros and cons of first- and second-order methods,\n",
        "we will look at Logistic Regression as an example. \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp7fKj9l7i_v"
      },
      "source": [
        "**Step 1**: You already have the gradient descent algorithm (use the line search version from PA1). Now,\n",
        "code up the Newton’s method algorithm as well. Write a function using the format below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFgovp0R7nek"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def nt(f,fp,fpp,y,A,xinit,maxit,tol):\n",
        "  '''\n",
        "  Note that you do no need to input a step size parameter\n",
        "  fpp is the function  handle  of  the  Hessian\n",
        "  '''\n",
        "  global nt_it\n",
        "  nt_it = 0\n",
        "  change = math.inf\n",
        "  x = xinit\n",
        "  while nt_it <= maxit and change > tol:\n",
        "    x_change = - np.linalg.inv(fpp(y, A, x)) @ fp(y, A, x)\n",
        "    new_x = x + x_change\n",
        "    change = abs(f(y, A, new_x) - f(y, A, x)) / abs(f(y, A, x))\n",
        "    nt_it += 1\n",
        "    x = new_x\n",
        "  return x\n",
        "\n",
        "def gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol):\n",
        "  \"\"\"\n",
        "  Note that ss changes to ss_init\n",
        "  ss_init is the starting stepsize for backtracking\n",
        "  \"\"\"\n",
        "  # Add your code here\n",
        "  global ls_it\n",
        "  ls_it = 0\n",
        "  x = xinit\n",
        "  ss = ss_init\n",
        "  change = math.inf\n",
        "  alpha, beta = 1, 1/2\n",
        "  while maxit >= ls_it and change > tol:\n",
        "    while f(y, A, x - ss * fp(y, A, x)) > f(y, A, x) - (alpha/2)*ss*(np.linalg.norm(fp(y, A, x))**2):\n",
        "      ss = beta * ss \n",
        "    x_change = - ss * fp(y, A, x)\n",
        "    new_x = x + x_change\n",
        "    change = abs(f(y, A, new_x) - f(y, A, x)) / abs(f(y, A, x))\n",
        "    ls_it += 1\n",
        "    x = new_x\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FtK3x9_73ui"
      },
      "source": [
        "**Note**: we will implement the basic version of Newton’s method (not BFGS or L-BFGS). We will also\n",
        "use the basic method to implement the Newton step by inverting the Hessian using np.linalg.inv.\n",
        "There are more efficient ways to do this inversion, but for the purpose of this assignment, do **NOT** use\n",
        "other ways even if you know them. The convergence criterion is the same as that in PA1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5M9CmXSYJle"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XzI3c1Y8qyk"
      },
      "source": [
        "**Step 2**: Code the objective function, the gradient, and the **Hessian** (you can use the Python lambda\n",
        "tool just like in PA1, or the regular function environment if you don’t like that). As a reminder, logistic\n",
        "regression has the following model\n",
        "$$y_i \\in \\{0,1\\}, \\quad p(y_i = 1) = \\text{sigmoid}(\\mathbf{a}_i^T \\mathbf{x}) =  \\frac{1}{1+e^{-\\mathbf{a}_i^T \\mathbf{x}}}, \\quad p(y_i=0) = 1 - p(y_i = 1).$$\n",
        "The likelihood can be written as \n",
        "$$p(y_i \\,|\\, \\mathbf{a}_i^T \\mathbf{x}) = p(y_i = 1 \\, | \\, \\mathbf{a}_i^T \\mathbf{x})^y_i \\cdot p(y_i = 0 \\, | \\, \\mathbf{a}_i^T \\mathbf{x})^{1-y_i} $$\n",
        "So we are solving the following optimization problem that minimizes the total negative log-likelihood\n",
        "minimize \\\\\n",
        "$$\\underset{\\mathbf{x}}{\\text{minimize}} \\quad \\quad  - \\sum_{i=1}^M (y_i \\log \\frac{1}{1+e^{-\\mathbf{a}_i^T \\mathbf{x}}} + (1-y_i) \\log \\frac{1}{1+e^{\\mathbf{a}_i^T \\mathbf{x}}}),$$\n",
        "or equivalently \n",
        "$$minimize_{\\mathbf{x}} \\sum_{i=1}^{M}\\log({1+e^{-(2y_i-1)a_i^T \\mathbf{x}}})$$\n",
        "where $y_i$ is the $i^{th}$ entry of the observation vector $\\mathbf{y}$ and $\\mathbf{a}_i$ is a column vector corresponding to the $i^{th}$ row of the matrix of covariates **A**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idDw8frDX8AV",
        "outputId": "f14e318e-8cc6-4cc1-c36f-d0ef98150418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 1]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "z = lambda y, A, x, c: np.exp(c*(2*y - np.full(y.shape, 1)) * (A @ x))\n",
        "f = lambda y, A, x: np.sum(np.log(1 + np.exp(-(2*y - np.full(y.shape, 1)) * (A @ x) )))\n",
        "fp = lambda y, A, x: -A.T @ ((2*y - np.full(y.shape, 1)) / (np.full(y.shape, 1) + z(y, A, x, 1)))\n",
        "fpp = lambda y, A, x: A.T @ (np.diag(np.full(y.shape, 1) / (np.full(y.shape, 2) + z(y, A, x, 1) + z(y, A, x, -1)))) @ A\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.full(3, 1)\n",
        "print(b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CtiLhndYLAK"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcIk0dJi8wAc"
      },
      "source": [
        "**Step 3**: Generate data. Set numpy’s random seed to 0. Then, generate the matrix of covariates\n",
        "**A** $\\in \\mathbb{R}^{M \\times N}$, which has i.i.d. entries distributed as N (0, 1). Use the same method, generate the\n",
        "regression coefficient vector $\\mathbf{x}$ $\\in$ $\\mathbb{R}^N$ as well. Then, generate the observation vector $\\mathbf{y} \\in \\{0, 1\\}^M$ using\n",
        "np.random.binomial()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niRn-hOtX9GM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "M= 200\n",
        "N = 5\n",
        "np.random.seed(0)\n",
        "gen_A = lambda M, N: np.random.normal(0, 1, (M, N))\n",
        "gen_x = lambda M, N: np.random.normal(0, 1, (N))\n",
        "gen_y = lambda M, N: np.random.binomial(1, 0.5, M)# weird function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnShPAW-YL8s"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5dZkzgGXEKo"
      },
      "source": [
        "**Step 4**: Run your code. Apply both algorithms to the logistic regression objective. The algorithm\n",
        "parameters (maxit and tol) should be the same as that in PA1. Note that for logistic regression,\n",
        "it could be difficult to choose the ss_init parameter. For this assignment, we will set it to be 400\n",
        "divided by the square of the maximum singular value of the matrix **A**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o9F_2ciX9x4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "\n",
        "M, N = 100, 20\n",
        "A = gen_A(M, N)\n",
        "x = gen_x(M, N)\n",
        "y = gen_y(M, N)\n",
        "U, D, vh = np.linalg.svd(A)\n",
        "ss_init = 400 / pow(np.amax(D), 2)\n",
        "maxit = 10000\n",
        "tol = 1e-15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNEGf3DJYN3X"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsAlQSOXXCZ"
      },
      "source": [
        "**1**) First, set M = 100 and N = 20. At convergence, print out the final cost, time taken, and number of\n",
        "iterations used by both algorithms. You should see that the final costs are more or less the same. Which\n",
        "algorithm is faster overall? Which algorithm converges in fewer iterations? Which algorithm has a longer\n",
        "per-iteration run time? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p_xMTr7X-cw",
        "outputId": "8c064e2f-7240-4789-dbf0-26c6ec2c8005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29.9 ms ± 5.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "cost for line search: -6.092102182577287\n",
            "iterations used for line search: 126\n",
            "3.89 ms ± 1.81 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "cost for Newton's method: -6.0921022711823065\n",
            "iterations used for Newton's method: 7\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "\n",
        "xinit = np.zeros(N)\n",
        "%timeit gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol)\n",
        "print(f\"cost for line search: {np.sum(x - gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol))}\")\n",
        "print(f\"iterations used for line search: {ls_it}\")\n",
        "%timeit nt(f,fp,fpp,y,A,xinit,maxit,tol)\n",
        "print(f\"cost for Newton's method: {np.sum(x - nt(f,fp,fpp,y,A,xinit,maxit,tol))}\")\n",
        "print(f\"iterations used for Newton's method: {nt_it}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMUAThakYOpe"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "Newton's method is faster overall and converges in fewer iterations; Newton's method has longer per-iteration runtime of 0.36ms whereas line search has a per-iteration run time of 0.21ms. Newton's method has a longer per-iteration runtime because it has to compute the Hessian and invert the matrix, which takes a lot of computation. Newton's method also takes less iteration to converge because it does a local quadratic approximation rather than a linear approximation like line search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsi7kNK0XYdE"
      },
      "source": [
        "**2**) Change the problem dimension to M = 10000 and N = 20. What happens now? How about in the\n",
        "case of N = 100? Which algorithm is faster overall? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PK3XOxSX_Km",
        "outputId": "e4d0e52c-ff77-441f-90dd-5a5d10527a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below are the results for M, N = 10000, 20:\n",
            "\n",
            "362 ms ± 63.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "cost for line search: -9.842804427382697\n",
            "iterations used for line search: 14\n",
            "1.73 s ± 135 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "cost for Newton's method: -9.842804479727027\n",
            "iterations used for Newton's method: 3\n",
            "\n",
            "\n",
            "Below are the results for M, N = 10000, 100:\n",
            "\n",
            "135 ms ± 28.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "cost for line search: -6.275225262012884\n",
            "iterations used for line search: 13\n",
            "1.67 s ± 15.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "cost for Newton's method: -6.275225265562029\n",
            "iterations used for Newton's method: 3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "M, N = 10000, 20\n",
        "A = gen_A(M, N)\n",
        "x = gen_x(M, N)\n",
        "y = gen_y(M, N)\n",
        "U, D, vh = np.linalg.svd(A)\n",
        "ss_init = 400 / pow(np.amax(D), 2)\n",
        "maxit = 10000\n",
        "tol = 1e-15\n",
        "\n",
        "xinit = np.zeros(N)\n",
        "print(\"Below are the results for M, N = 10000, 20:\\n\")\n",
        "%timeit gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol)\n",
        "print(f\"cost for line search: {np.sum(x - gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol))}\")\n",
        "print(f\"iterations used for line search: {ls_it}\")\n",
        "%timeit nt(f,fp,fpp,y,A,xinit,maxit,tol)\n",
        "print(f\"cost for Newton's method: {np.sum(x - nt(f,fp,fpp,y,A,xinit,maxit,tol))}\")\n",
        "print(f\"iterations used for Newton's method: {nt_it}\")\n",
        "\n",
        "M, N = 10000, 20\n",
        "A = gen_A(M, N)\n",
        "x = gen_x(M, N)\n",
        "y = gen_y(M, N)\n",
        "U, D, vh = np.linalg.svd(A)\n",
        "ss_init = 400 / pow(np.amax(D), 2)\n",
        "print(\"\\n\")\n",
        "print(\"Below are the results for M, N = 10000, 100:\\n\")\n",
        "%timeit gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol)\n",
        "print(f\"cost for line search: {np.sum(x - gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol))}\")\n",
        "print(f\"iterations used for line search: {ls_it}\")\n",
        "%timeit nt(f,fp,fpp,y,A,xinit,maxit,tol)\n",
        "print(f\"cost for Newton's method: {np.sum(x - nt(f,fp,fpp,y,A,xinit,maxit,tol))}\")\n",
        "print(f\"iterations used for Newton's method: {nt_it}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F54MOgNXYPiT"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "For the case M, N = 10000, 20, Newton's method uses significantly more time, but still converges with fewer iterations than line search, line search has better per-iteration runtime than Newton's method with 7.6 ms compared to 226 ms. When N becomes 100, Newton's method takes even more time per-iteration. \n",
        "\n",
        "Newton's method still takes less iterations to converge; however, the number of computations and required time for each iteration become drastically greater as the size of matrix gets bigger, taking more time to converge than line search.\n",
        "\n",
        "Therefore, line search is faster overall when the matrix size is big.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfPdaRf_XZvc"
      },
      "source": [
        "**3**) Load the given data file noisy1.npz. Select one algorithm to recover $\\mathbf{x}$ using the same approach as that in PA1. Which algorithm are you going to select? Why? Reshape it to a $40 \\times 40$ matrix. Visualize it as an image in grayscale using matplotlib. What do you think is the original image? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKHrsIJPYAAx",
        "outputId": "fb9fa6e8-cf96-4355-ab30-584c16a3e1e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5000, 1600)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd1b6deabd0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "data = np.load(\"noisy1.npz\")\n",
        "y = data['y']\n",
        "A = data['A']\n",
        "print(A.shape)\n",
        "xinit = np.zeros(A.shape[1])\n",
        "ss_init = 1\n",
        "tol = 1e-4\n",
        "maxit = 10000\n",
        "solution = gd_ls(f,fp,y,A,xinit,ss_init,maxit,tol)\n",
        "solution = solution.reshape((40,40))\n",
        "plt.gray()\n",
        "plt.imshow(solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61_K3o1vYQT5"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "It's Mario\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjG-RWk5BGd"
      },
      "source": [
        "#Add Colab link here: # \n",
        "\n",
        "https://colab.research.google.com/drive/1rFUwyFutMmVGfWxo76b7lph6PJaZqagN?authuser=1#scrollTo=AZjG-RWk5BGd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}