{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QFrankQ/Applied-Numerical-Optimization/blob/main/ADMM_and_Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPSqaNgZ5Cht"
      },
      "source": [
        "#**Collaborative Filtering**#\n",
        "In this programming assignment, we will implement a matrix completion method that is one approach to collaborative filtering.  \\\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp7fKj9l7i_v"
      },
      "source": [
        "**Step 1**: Step 1: Generate data. Recall that in collaborative filtering, you are given a matrix of user ratings for items. Say the matrix is $M \\times N$, where $M$ is the number of users and $N$ is the number of items. \n",
        "\n",
        "A key observation in collaborative filtering is that users and items can be roughly grouped into a small number of clusters; so the matrix has some intrinsic ``low-rankness'', with a rank $K$ that is much less than $M$ and $N$. Now, set $M=100$,  $N=50$, and $K=5$, and set the random seed to zero. Generate two factor matrices of dimensions $M \\times K$ and $K \\times N$, respectively, using *np.random.uniform()*. Now multiply them together, you get a matrix $\\mathbf{Z}$ of size $M \\times N$, but with rank $K$. \n",
        "\n",
        "When a user rates an item, they typically give an integer rating (how many stars out of 5, for example). Now, scale the entries of $\\mathbf{Z}$ so that they are in the range $[-2,2]$, and then round them to the nearest integer, i.e., each rating is one in $\\{-2,-1,0,1,2\\}$. \n",
        "\n",
        "Now create a new matrix $\\mathbf{Y}$; if $\\mathbf{Z}$ contains the rating **every** user would give to **every** item, then we use $\\mathbf{Y}$ to characterize the actual ratings we observe, since not every user rates every item. Construct $\\mathbf{Y}$ by randomly taking out 80\\% of the entries in $\\mathbf{Z}$; set these entries to a number that represents \"missing\", e.g., $-$10; they are not observed. The rest of the entries in $\\mathbf{Y}$ are equal to those in $\\mathbf{Z}$. Denote $\\Omega$ as the set of indices (row, column) that corresponds to observed entries in $\\mathbf{Y}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKCUE3PXyxa4"
      },
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "M = 100\n",
        "N = 50\n",
        "K = 5\n",
        "np.random.seed(0)\n",
        "MK = np.random.uniform(size = (M,K))\n",
        "KN = np.random.uniform(size = (K,N))\n",
        "Z = MK @ KN\n",
        "max_entry = np.amax(Z)\n",
        "min_entry = np.amin(Z)\n",
        "diff = max_entry-min_entry\n",
        "resize = lambda x: -2 + 4*(x-min_entry)/diff\n",
        "scaled_Z = resize(Z)\n",
        "Z = np.rint(scaled_Z)\n",
        "indices = np.random.choice(2, size=(M,N), replace=True, p=[0.8, 0.2])\n",
        "Y = Z.copy()\n",
        "Y[indices == 0] = -10\n",
        "mask = np.where(indices == 1, indices, 0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk68zU2eyyLg"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5M9CmXSYJle"
      },
      "source": [
        "**Step 2:** Set up our problem. Say we solve the following optimization problem\n",
        "\\begin{align*}\n",
        "\\underset{\\mathbf{X}}{\\text{minimize}} & \\quad \\sum_{(i,j) \\in \\Omega} \\frac{1}{2} (X_{i,j} - Y_{i,j})^2 + \\tau \\| \\mathbf{X} \\|_* \\\\\n",
        "\\text{subject to} & \\quad \\| \\mathbf{X} \\|_\\infty \\leq a,\n",
        "\\end{align*} \n",
        "where $X_{i,j}$ denotes the $(i,j)$-th entry of $\\mathbf{X}$, $\\| \\cdot \\|_*$ denotes the nuclear norm of a matrix. i.e., sum of its singular values, and $\\| \\cdot \\|_\\infty$ denotes the infinity norm of a matrix, i.e., the largest absolute value of its entries. $\\tau > 0$ and $a > 0$ are constants. The nuclear norm penalty promotes low-rankness of $\\mathbf{X}$; the infinity norm constraint makes sure that the entries of $\\mathbf{X}$ are bounded. \n",
        "\n",
        "Now you see that solving this problem does not seem easy; there are two annoying things, one is the nuclear norm, the other is the infinity norm constraint. Dealing with them together at the same time seems quite hard, since we do not know how to do a proximal operator for both. Instead, we use the idea of ADMM and use the strategy of divide and conquer: \n",
        "\\begin{align*}\n",
        "\\underset{\\mathbf{X},\\mathbf{Z}}{\\text{minimize}} & \\quad \\sum_{(i,j) \\in \\Omega} \\frac{1}{2} (X_{i,j} - Y_{i,j})_2^2 + \\tau \\| \\mathbf{Z} \\|_* \\\\\n",
        "\\text{subject to} & \\quad \\| \\mathbf{X} \\|_\\infty \\leq a, \\mathbf{X} = \\mathbf{Z}.\n",
        "\\end{align*} \n",
        "Note: as we discussed in class, you can swap the places where $\\mathbf{X}$ and $\\mathbf{Z}$ show up in the nuclear norm regularization/infinity norm constraint.  \n",
        "\n",
        "We now see a (rather) simple solution strategy at our hands using ADMM. Overall, the subproblem for $\\mathbf{X}$ can be solved by proximal gradient; the gradient can be derived from the augmented Lagrangian, and the proximal operator is the same as that in the 1-D total variation denoising dual problem. The subproblem for $\\mathbf{Z}$ involves a nuclear norm penalty, but the solution is also something you've done before: it is simply the proximal operator in PA4 applied to the vector of singular values. Implement this by taking the singular value decomposition (SVD), put the vector of singular values through the proximal operator, and reconstruct the matrix. \\\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcIk0dJi8wAc"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**Step 3)**\n",
        "Implement these components. The main algorithm should have the following format:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niRn-hOtX9GM"
      },
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "import math\n",
        "#objective for the x sub-problem\n",
        "#g = lambda X, Y, Z, omega, gamma, t: 1/2 * sum([pow((X[row][column] - Y[row][column]),2) for row, column in omega]) + t/2 * pow(np.linalg.norm((X - Z + gamma), ord = 'fro'), 2)\n",
        "\n",
        "g = lambda X, Y, Z, gamma, t, mask: 1/2 * np.sum(np.square(X-Y)* mask) + t/2 * pow(np.linalg.norm((X - Z + gamma), ord = 'fro'), 2)\n",
        "#gradient for the x sub-problem\n",
        "# def gp(X, Y, Z, gamma, t):\n",
        "#   grad = np.full((M,N), 0)\n",
        "#   for i in range(M):\n",
        "#     for j in range(N):\n",
        "#       grad[i][j] = t * (X[i][j]-Z[i][j]+gamma[i][j])     \n",
        "#   for (i, j) in omega:\n",
        "#     grad[i][j] += (X[i][j] - Y[i][j])\n",
        "#   return grad\n",
        "\n",
        "gp = lambda X, Y , Z, gamma, t, mask: t * (X - Z + gamma) + (X-Y)* mask\n",
        "\n",
        "#proximal operator for the x sub-problem\n",
        "gprox = lambda A, a: np.maximum(np.minimum(A, a), -a)\n",
        "  \n",
        "#z sub-problem\n",
        "#z sub-problem objective\n",
        "def f(X, Z, gamma, tau, t):\n",
        "   return 1/2 * pow(np.linalg.norm((X - Z + gamma), ord = 'fro'), 2) + tau/t * np.linalg.norm(Z, ord = 'nuc')\n",
        "\n",
        "#soft-threshold on singular values of X+gamma\n",
        "def fprox(A,tau,t):\n",
        "  u, s, v = np.linalg.svd(A,full_matrices=False)\n",
        "  sp = np.maximum(s-tau/t,0)\n",
        "  return np.dot(u,np.dot(np.diag(sp),v))\n",
        "  \n",
        "def mc_admm(Y,lam,a,maxit,tol):\n",
        "    #Y is the input matrix\n",
        "    #lam and a are the parameters\n",
        "    #maxit and tol help you terminate the iterations\n",
        "    t = 2 #stepsize\n",
        "    it = 0\n",
        "    X_change = math.inf\n",
        "    Z_change = math.inf\n",
        "    X = np.full((M,N), 0)\n",
        "    Z = np.full((M,N), 0)\n",
        "    gamma = np.full((M,N), 0)\n",
        "\n",
        "    #X subproblem\n",
        "    inner_maxit = 100\n",
        "    inner_tol = 1e-3\n",
        "    \n",
        "    while it <= maxit and (X_change > tol or Z_change > tol) :\n",
        "      X_inner_change = math.inf\n",
        "      x_it = 0\n",
        "      ss = 1/(1+t)\n",
        "      X_prev = X\n",
        "      Z_prev = Z\n",
        "      while x_it <= inner_maxit and X_inner_change > inner_tol:\n",
        "        A = X - ss * gp(X, Y, Z, gamma, t, mask)\n",
        "        new_X = gprox(A, a)\n",
        "        old_obj = g(X, Y, Z, gamma, t, mask)\n",
        "        X_inner_change = abs(g(new_X, Y, Z, gamma, t, mask) - old_obj)/ abs(old_obj)\n",
        "        x_it += 1\n",
        "        X = new_X\n",
        "      #Z sub-problem\n",
        "      new_Z = fprox(X + gamma, lam, t)\n",
        "      Z = new_Z\n",
        "      gamma = gamma + X - Z\n",
        "      it += 1\n",
        "\n",
        "      #print(f\"lam: {lam}, it: {it}\")\n",
        "      X_prev_F = np.linalg.norm(X_prev, ord = 'fro')\n",
        "      Z_prev_F = np.linalg.norm(Z_prev, ord = 'fro')\n",
        "      X_change = np.linalg.norm((new_X - X_prev), ord = 'fro')\n",
        "      Z_change = np.linalg.norm((new_Z - Z_prev), ord = 'fro')\n",
        "      # print(f\"{lam} {np.amax(Z)}\")\n",
        "      # print(f\"{lam} {np.amin(Z)}\")\n",
        "      # X_prev_F = X_F\n",
        "      # Z_prev_F = Z_F\n",
        "      #print(f\"X_change {X_change}\")\n",
        "      # X_boo = X_change > tol\n",
        "      # Z_boo = Z_change > tol\n",
        "      # print(f\"X_change: {X_change}, Z_change: {Z_change}\")\n",
        "      # print(f\"X_F: {X_prev_F}, Z_F: {Z_prev_F}\")\n",
        "      # print(f\"x_boo: {X_boo}, z_boo: {Z_boo}\\n\")\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5dZkzgGXEKo"
      },
      "source": [
        "What you have to implement include: variable initialization, solutions to subproblems, and convergence check. To check convergence, for simplicity, you can compare the values of the matrices $\\mathbf{X}$ and $\\mathbf{Z}$ in consecutive iterations, if the relative change (in terms of Frobenius norm) is small (read: less than tol), you can terminate the iterations. \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNEGf3DJYN3X"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znDF3ix05aAD"
      },
      "source": [
        "**Step 4a)** Run your code. Tweak the algorithm parameters (maxit and tol); for the outer iterations, set them to $1000$ and $1e-6$, and for the inner iterations, set them to $100$ and $1e-3$. A good value for $t$, the ADMM stepsize parameter, is $2$. Calculate a suitable constant stepsize for your proximal gradient algorithm to solve the subproblem for $X$ using Lipschitz continuity. Set $a=2$ since that's how the data is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "add43_el5xXC"
      },
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "a = 2\n",
        "maxit = 1000\n",
        "tol = 1e-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vShabn6G5xrS"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "The Lipschitz constant for proximal gradient of the X-subproblem would be t(8 + the maximum entry of gamma) as the difference of X_ij and Z_ij is at most 4 (ratings) and the difference of X_ij and Y_ij is also 4, and gamma_ij is bounded by the greatest entry of gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLWRRQWt5yTI"
      },
      "source": [
        "**Step 4b)** For different values of $\\tau \\in \\{0, 0.3, 1, 2, 3, 4, 5, 10\\}$, visualize the singular values of the final solution for $\\mathbf{X}$. What is the impact of $\\tau$? Explain. Note that the solution will not be perfectly low-rank (numerical reasons), but you can treat the very small ($\\sim 1e-6$) singular values as essentially zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQHswtS857F1"
      },
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axs = plt.subplots(8, figsize = (15, 50))\n",
        "# fig.tight_layout(pad = 5.0)\n",
        "# i = 0\n",
        "tau_list = [0, 0.3, 1, 2, 3, 4, 5, 10]\n",
        "\n",
        "fig, axs = plt.subplots(8, figsize = (15, 30))\n",
        "i = 0\n",
        "for lam in tau_list:\n",
        "    X = mc_admm(Y,lam,a,maxit,tol)\n",
        "    u, s, v = np.linalg.svd(X)\n",
        "    axs[i].plot(s, 'o', label = 'singular values for X')\n",
        "    axs[i].set_title(f'Lambda = {lam}')\n",
        "    i+= 1\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbK9sfcf572i"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "$\\tau$ forces the recovered matrix to be low rank.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN6RjV9358C9"
      },
      "source": [
        "**Step 4c)** Set $\\tau = 5$. Visualize the singular values of $\\mathbf{X}$ and the maximum absolute value among the entries of $\\mathbf{Z}$ for some iterations before convergence (say you converge in 500 iterations, visualize them for iteration 450, 460, 470, 480, 490, 500) and look at the trend. What can you say about the two subproblems interacting with each other? \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlipaOSt6BCA"
      },
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "def mc_admm_alt(Y,lam,a,maxit,tol):\n",
        "    #Y is the input matrix\n",
        "    #lam and a are the parameters\n",
        "    #maxit and tol help you terminate the iterations\n",
        "    t = 2 #stepsize\n",
        "    it = 0\n",
        "    X_change = math.inf\n",
        "    Z_change = math.inf\n",
        "    X = np.full((M,N), 0)\n",
        "    Z = np.full((M,N), 0)\n",
        "    gamma = np.full((M,N), 0)\n",
        "\n",
        "\n",
        "    all_s = []\n",
        "    all_abs = [] \n",
        "    #X subproblem\n",
        "    inner_maxit = 100\n",
        "    inner_tol = 1e-3\n",
        "    \n",
        "    while it <= maxit and (X_change > tol or Z_change > tol) :\n",
        "      X_inner_change = math.inf\n",
        "      x_it = 0\n",
        "      ss = 1/(1+t)\n",
        "      X_prev = X\n",
        "      Z_prev = Z\n",
        "      while x_it <= inner_maxit and X_inner_change > inner_tol:\n",
        "        A = X - ss * gp(X, Y, Z, gamma, t, mask)\n",
        "        new_X = gprox(A, a)\n",
        "        old_obj = g(X, Y, Z, gamma, t, mask)\n",
        "        X_inner_change = abs(g(new_X, Y, Z, gamma, t, mask) - old_obj)/ abs(old_obj)\n",
        "        x_it += 1\n",
        "        X = new_X\n",
        "      #Z sub-problem\n",
        "      new_Z = fprox(X + gamma, lam, t)\n",
        "      Z = new_Z\n",
        "      gamma = gamma + X - Z\n",
        "      it += 1\n",
        "\n",
        "      X_prev_F = np.linalg.norm(X_prev, ord = 'fro')\n",
        "      Z_prev_F = np.linalg.norm(Z_prev, ord = 'fro')\n",
        "      X_change = np.linalg.norm((new_X - X_prev), ord = 'fro')\n",
        "      Z_change = np.linalg.norm((new_Z - Z_prev), ord = 'fro')\n",
        "      X_boo = X_change > tol\n",
        "      Z_boo = Z_change > tol\n",
        "      # print(f\"Xmax: {np.amax(X)}\")\n",
        "      # print(f\"Xmin: {np.amin(X)}\")\n",
        "      # print(f\"Zmax: {np.amax(Z)}\")\n",
        "      # print(f\"Zmin: {np.amin(Z)}\")\n",
        "      u, s, v = np.linalg.svd(X)\n",
        "      all_s.append(s)\n",
        "      all_abs.append(max(abs(np.amax(Z)), abs(np.amin(Z))))\n",
        "    return X, all_s, all_abs\n",
        "tau = 5\n",
        "#fig, axs = plt.subplots(1, figsize = (15, 30))\n",
        "maxit = 10000\n",
        "tol = 1e-6\n",
        "X, all_s, all_abs = mc_admm_alt(Y,tau,2,maxit,tol)\n",
        "\n",
        "fig, axs = plt.subplots(7, figsize = (15, 30))\n",
        "last_six_abs = []\n",
        "for i in range(0, 6)[::-1]:\n",
        "  print(all_abs[-(i*10+1)])\n",
        "  last_six_abs.append(all_abs[-(i*10+1)])\n",
        "  axs[i].plot(all_s[-i*10], 'o')\n",
        "  axs[i].set_title(f\"singular value of X at {i*10}th to last iteration\")\n",
        "\n",
        "axs[6].plot(last_six_abs[::-1], 'o')\n",
        "axs[6].ticklabel_format(useOffset=False)\n",
        "#axs[6].set_xlim([5, 0])\n",
        "axs[6].set_title(f\"the maximum absolute value in the last 50, 40, 30, 20, 10, 0 iterations\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSIdBhEG6BNE"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "The singular values became low rank and the absolute maximum entries in Z approaches the boundary of entries, which is 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrIFjZPH6CxT"
      },
      "source": [
        "**Step 4d)** Set $\\tau = 2$. Let's say the columns (movies) corresponds to the bottom 50 movies rated by IMDB users: see the bottom 100 at\n",
        "[url](https://www.imdb.com/chart/bottom?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=4da9d9a5-d299-43f2-9c53-f0efa18182cd&pf_rd_r=QFNQ592R0V2AX6RPXYJN&pf_rd_s=right-4&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_ql_8). Look at the $7$-th user (the $7$-th row in the matrix). Among the movies they haven't rated, which 3 movies will you predict them to like the most?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hP9xRwZ7CWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c674cc92-ff34-4f2f-fd8e-1bcacf69e1cd"
      },
      "source": [
        "\"\"\"\n",
        "  Add your code here\n",
        "\"\"\"\n",
        "tau = 2\n",
        "X = mc_admm(Y,tau,2,maxit,tol)\n",
        "\n",
        "predict = X + (-10)* mask\n",
        "top_three = np.argpartition(predict[6],-3)[-3:]\n",
        "print(top_three)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35 41 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAnBvPK7Ce7"
      },
      "source": [
        "*Answer the questions and discuss your findings here*\n",
        "\n",
        "The indices are 35, 41, 19. The predicted movies are Turks in Space, 365 days, and Kazaam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjG-RWk5BGd"
      },
      "source": [
        "#Add Colab link here: # \n",
        "\n",
        "https://colab.research.google.com/drive/1ey2uQEthcdiMgd4MAO-myZqjM7-NOkyA?authuser=1#scrollTo=iqAnBvPK7Ce7"
      ]
    }
  ]
}